[{"title":"Overview","type":0,"sectionRef":"#","url":"/","content":"Overview Welcome to the SUSE Edge Engineering docs site.","keywords":""},{"title":"Rancher portfolio disambiguation","type":0,"sectionRef":"#","url":"/misc/rancher-disambiguation","content":"","keywords":""},{"title":"TL;DR​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#tldr","content":"RKE1, RKE2 and K3s are flavours of Kubernetes, Rancher Manager can be used to manage and provision different deployments of Kubernetes itself with a primary focus on RKE1/RKE2, Fleet can watch Git Repositories, detect changes and tell Kubernetes what it needs to be running, Elemental considers a specific approach to provisioning Kubernetes in Edge scenarios where the provisioning can be preloaded at the OS level for Rancher Manager to control later "},{"title":"Rancher​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#rancher","content":"Rancher (or Rancher Manager) is a multi cluster management solution for provisioning, managing and accessing multiple downstream kubernetes clusters. To provision new clusters Rancher can interact with different infrastructure and virtualization tools (vSphere/AWS etc) as an api client, request VMs and networks and setup a kubernetes cluster inside of those, it also works with bare metal machines by generating an join command you an run each time. "},{"title":"Fleet​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#fleet","content":"Fleet is usually a component of Rancher (although it can be used independently) that allows you to use a GitOps workflow for multi-cluster (i.e it allows you to define your git repositories and the clusters they should apply to at the management cluster level). "},{"title":"Elemental​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#elemental","content":"Elemental is a way to automatically deploy/register new clusters and manage the OS of their node, you can define clusters and their nodes on the management cluster then generate an OS installer image, when booting your node from that image it will install the node, register it to the manager and configure it for its role in the local cluster. This is the SUSE/Rancher way of doing zero touch provisioning. Elemental takes a different view of cluster provisioning focused on Edge deployments, typically Rancher services datacentre deployments of Kubernetes with enterprise servers etc; in an Edge scenario e.g. factory or cruise ship theres no guarantee of access for Rancher to contact and provision a cluster directly (i.e. limited bandwidth, firewalls etc) - Elemental instead is used to preload an operating system with all the information needed to set the cluster up, you can install that into the servers that you want to cluster and then it will reach back to Rancher to be under management at that point "},{"title":"Kubernetes​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#kubernetes","content":"Kubernetes as a standard and core technology is really a cross industry effort like Linux and has become core to DevOps as a cultural movement - as it enables defining and deploying your infrastructure as code and with lots of automation for extensive business continuity and high availability Kubernetes is a receptacle though - it runs what you tell it to run, usually people use automation to tell it what to do and this requires some kind of application to detect application configuration and apply it to Kubernetes - usually this is fulfilled through developer pipelines (CI/CD) where things are deployed as they are developed "},{"title":"Kubernetes distributions​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#kubernetes-distributions","content":"Kubernetes Distributions, like Linux OSes, come in different flavours, RKE and RKE2 are two different flavours of Kubernetes in this manner; but like Ubuntu vs SUSE do for an OS they are ultimately just packaging an implementation of Kubernetes. Other examples include EKS,AKS and GKE which are flavours produced by AWS, Azure and GCP respectively. When we say a kubernetes cluster we mean a specific instance of a distribution installed on servers that are managed as a group (each server being a node in the cluster) "},{"title":"K3Ss​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#k3ss","content":"K3s is a fully compliant and lightweight Kubernetes distribution focused on Edge, IoT, ARM or just for situations where a PhD in K8s clusterology is infeasible "},{"title":"RKE (or RKE1)​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#rke-or-rke1","content":"Rancher Kubernetes Engine is a Kubernetes distribution that uses an older architecture and relies on Docker Engine to run containers "},{"title":"RKE2​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#rke2","content":"RKE2 also known as RKE Government, is Rancher's next-generation Kubernetes distribution that uses a newer architecture based on ContainerD. RKE2 combines the best-of-both-worlds from the 1.x version of RKE (hereafter referred to as RKE1) and K3s. From K3s, it inherits the usability, ease-of-operations, and deployment model. From RKE1, it inherits close alignment with upstream Kubernetes. In places K3s has diverged from upstream Kubernetes in order to optimize for edge deployments, but RKE1 and RKE2 can stay closely aligned with upstream. "},{"title":"Rancher vs K3s vs RKE​","type":1,"pageTitle":"Rancher portfolio disambiguation","url":"/misc/rancher-disambiguation#rancher-vs-k3s-vs-rke","content":"You don’t need Rancher to set up K3s or RKE1 or RKE2 on their own it just makes the whole process easier. Rancher runs as a Management Interface that can interact with running clusters and also provision new clusters - as well as manage authentication to the downstream clusters, and it can also do other things like interact with applications that kubernetes is orchestrating and provides monitoring tools "},{"title":"SLE Micro vs SLE Micro for Rancher","type":0,"sectionRef":"#","url":"/misc/slemicro-vs-slemicro-rancher","content":"","keywords":""},{"title":"SLE Micro​","type":1,"pageTitle":"SLE Micro vs SLE Micro for Rancher","url":"/misc/slemicro-vs-slemicro-rancher#sle-micro","content":"SLE Micro is a minimal operating system that is specifically designed for use in containerized environments. It is based on the concept of a transactional server, where the entire operating system is treated as a single, immutable unit. This means that any changes to the system are made through atomic transactions, which can be rolled back if necessary. This approach provides increased security, reliability, and consistency, making it ideal for use in production environments. It includes only the essential components required to run container workloads and has a small footprint, making it ideal for running in resource-constrained environments such as edge devices or IoT devices. SLE Micro can be used as a single-node container host, Kubernetes cluster node, single-node KVM virtualization host or in public cloud. One of the main benefits of using SLE Micro is its open standards design, which allows users to explore commodity hardware from several vendors and build an open source-based software platform. This enables significant cost savings on both software and hardware while keeping full control of the technology stack strategy and roadmap. One example for the usage would be Telecom where SLE Micro is helping them unlock the cost-savings potential of open-source design for both software and hardware. With the open standards design, they can explore commodity hardware from several vendors and build an open source-based software platform using open standards such as Kubernetes with open source tools of their choice. Ultimately, they expect significant savings on software and hardware, while keeping full control of their technology stack strategy and roadmap. For more info and steps on how to use SLE micro you can check thefollowing "},{"title":"SLE Micro for Rancher​","type":1,"pageTitle":"SLE Micro vs SLE Micro for Rancher","url":"/misc/slemicro-vs-slemicro-rancher#sle-micro-for-rancher","content":"SLE Micro for Rancher is a variant of SLE Micro that is specifically designed to work with containerized workloads in a Rancher environment. It is built around the Elemental platform, which provides additional features and tools that make it easier to deploy and manage container workloads. One of the main differences between SLE Micro for Rancher and SLE Micro is the preconfigured networking and storage options that come with it. This simplifies the process of setting up a Rancher environment and ensures that everything is configured correctly for container workloads. SLE Micro for Rancher also includes integrated monitoring and logging capabilities, which make it easier to monitor the health and performance of your container workloads. This can help you identify and resolve issues quickly. Another difference is that it is not strictly a transactional server, but it does include transactional features. For example, it uses the Btrfs file system, which is also used in the SLE Micro and supports snapshots and rollbacks, allowing you to easily revert to a previous system state if needed. Additionally, updates to the system are delivered through a transactional update mechanism, which ensures that the system remains in a consistent state throughout the update process. See more here "},{"title":"Intro","type":0,"sectionRef":"#","url":"/quickstart/k3s-on-slemicro","content":"","keywords":""},{"title":"K3s​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s","content":"K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. It is packaged as a single and small binary so installations and updates are fast and easy. The installation procedure can be as simple as downloading the k3s binary and run it. However, the prefered way is to use the install script as it creates and configures a service. The script supports different installation parameters to customize K3s, including HA support, install control-plane nodes, dedicated etcd nodes, agents, etc. Once installed, the parameters and flags can be modified, added or removed just by changing the systemd unit file or the config file and restarting the service. Neat! "},{"title":"K3s on SLE Micro​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s-on-sle-micro","content":"The installation scripts supports SLE Micro, it recognizes the underlying operating system, installs the k3s-selinux package using transactional-updates and creates the k3s or k3s-agent services. NOTE: On SLE Micro, the install script doesn't start the k3s or k3s-agent service (ideally you should reboot the host once you run a transactional-update), but this can be override by using the INSTALL_K3S_SKIP_START=false environment variable. "},{"title":"K3s all-in-one​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s-all-in-one","content":"The simplest way to run K3s is an all-in-one server (not suited for production environments) is by running: curl -sfL https://get.k3s.io | sh -  A few environment variables to tweak our installation can be used as well as: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --cluster-init --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -  The settings can be specified either as environment variables, command line flags, a configuration file, or both, it is just a personal choice: curl -sfL https://get.k3s.io | sh -s - server --token foobar --cluster-init --write-kubeconfig-mode=644  write-kubeconfig-mode: &quot;0644&quot; cluster-init: true token: &quot;foobar&quot;  In this example: write-kubeconfig-mode is self explanatory (the default is 0600)cluster-init enables clustering by deploying an embedded etcd databasetoken a random token is generated to be able to add nodes to the cluster, specifying it at installation time makes things easier as it is known upfront The official documentation explains all the flags in detail. "},{"title":"Adding agents​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#adding-agents","content":"Adding an agent is as simple as running the install script with a few parameters, including the URL of the cluster as: curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=foobar sh -  "},{"title":"K3s HA​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s-ha","content":"The easiest way to run a K3s HA cluster is by installing a first node using the --cluster-init flag and then, start adding nodes. # First node curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --cluster-init --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -  # Rest of the nodes curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --server https://myserver:6443 --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -  Agent nodes can be added as well: curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;agent --server https://myserver:6443&quot; K3S_TOKEN=foobar sh -  This is what a cluster with 3 control-plane nodes and 2 agents look like: NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cp01 Ready control-plane,etcd,master 2m26s v1.26.4+k3s1 192.168.205.99 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 cp02 Ready control-plane,etcd,master 98s v1.26.4+k3s1 192.168.205.100 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 cp03 Ready control-plane,etcd,master 71s v1.26.4+k3s1 192.168.205.101 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 w01 Ready &lt;none&gt; 63s v1.26.4+k3s1 192.168.205.102 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1 w02 Ready &lt;none&gt; 39s v1.26.4+k3s1 192.168.205.103 &lt;none&gt; SUSE Linux Enterprise Micro 5.4 5.14.21-150400.24.46-default containerd://1.6.19-k3s1  "},{"title":"K3s API HA​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s-api-ha","content":"The previous section lacks an important detail, the Kubernetes API is served by the 3 control-plane nodes, but the API certificate is generated just for the first node. If the first node is down, the clients needs their API endpoint to be tweaked to point to another node (i.e.- for kubectl, using the -s flag or modifying the kubeconfig file) and the certificate won't be accepted as it doesn't contain the IP/hostname of that other node (it can be forced to be ignored using --insecure-skip-tls-verify=true for kubectl but that's not a good practice). Ideally a mechanism to expose the K3s API in a high availability scenario is required. This usually means running a load balancer outside of the K3s cluster to serve and redirect the requests to the K3s API endpoints, so if one of the servers fail, the load balancer will re-route the requests to the other ones. This solves the HA problem but it adds complexity as it requires an external service, which sometimes is not available (typically in non-cloud environments such as baremetal deployments). One approach can be to run a self-contained solution involving kube-vip to expose the K3s API over a virtual IP (optionally including a load balancer as well). This solves the HA problem but the certificate can still be a problem... but K3s got you covered. By using the --tls-san flag at K3s installation time, a list of IPs and/or hostnames can be provided for the certificate to be included as Subject Alternative Names, meaning the K3s API will be happily served from those IPs/hostnames, and if those are the ones being served by the VIP, the solution is now HA and certificate-proof! Let's see it in more detail in the next section. NOTE: kube-vip can be used also to expose Kubernetes services, but this is out of scope of this document. "},{"title":"VIP reservation​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#vip-reservation","content":"The VIP needs to be an IP available in the same subnet than the one where the control plane hosts are running (this is technically not true for the VIP itself but for load-balancing). NOTE: If you are using OSX to virtualize the SLE Micro OS where K3s is going to be installed, you can see the dhcp leases in the /var/db/dhcpd_leases file and the subnet range in the /Library/Preferences/SystemConfiguration/com.apple.vmnet.plist one. You can use a free IP in that range, but if you find a way to reserve an IP in that range, please open a GitHub issue or a pull request with instructions to do it!. "},{"title":"K3s installation - First node​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s-installation---first-node","content":"The first step is to install K3s in HA and using the --tls-san flag as well. This flag can be repeated many times, so in this example will be used to add both the IP (192.168.205.10 in this example) and the DNS name of the VIP (using sslip.io as a poor's man DNS): curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --cluster-init --write-kubeconfig-mode=644 --tls-san=192.168.205.10 --tls-san=https://192.168.205.10.sslip.io&quot; K3S_TOKEN=foobar sh -  The rest of the nodes will be installed after kube-vip as the server URL for them to join the cluster will be the VIP. "},{"title":"Kube-vip installation​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#kube-vip-installation","content":"The official kube-vip documentation explains the steps in more detail, but essentially it means creating the required resource files for kube-vip to run (RBAC and a DaemonSet) and leveraging K3s auto-deploy feature (aka. any manifest stored in a particular folder of the host /var/lib/rancher/k3s/server/manifests will be automatically deployed at the K3s service startup or when the file changes via something similar to kubectl apply -f) kube-vip will be executed. NOTE: In this case, the --services flag for kube-vip won't be used. export VIP=192.168.205.10 cat &lt;&lt;- EOF &gt; /var/lib/rancher/k3s/server/manifests apiVersion: v1 kind: ServiceAccount metadata: name: kube-vip namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: &quot;true&quot; name: system:kube-vip-role rules: - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;, &quot;services/status&quot;, &quot;nodes&quot;, &quot;endpoints&quot;] verbs: [&quot;list&quot;,&quot;get&quot;,&quot;watch&quot;, &quot;update&quot;] - apiGroups: [&quot;coordination.k8s.io&quot;] resources: [&quot;leases&quot;] verbs: [&quot;list&quot;, &quot;get&quot;, &quot;watch&quot;, &quot;update&quot;, &quot;create&quot;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-vip-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-vip-role subjects: - kind: ServiceAccount name: kube-vip namespace: kube-system --- apiVersion: apps/v1 kind: DaemonSet metadata: labels: app.kubernetes.io/name: kube-vip-ds app.kubernetes.io/version: v0.5.12 name: kube-vip-ds namespace: kube-system spec: selector: matchLabels: app.kubernetes.io/name: kube-vip-ds template: metadata: labels: app.kubernetes.io/name: kube-vip-ds app.kubernetes.io/version: v0.5.12 spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/master operator: Exists - matchExpressions: - key: node-role.kubernetes.io/control-plane operator: Exists containers: - args: - manager env: - name: vip_arp value: &quot;true&quot; - name: port value: &quot;6443&quot; - name: vip_interface value: eth0 - name: vip_cidr value: &quot;32&quot; - name: cp_enable value: &quot;true&quot; - name: cp_namespace value: kube-system - name: vip_ddns value: &quot;false&quot; - name: vip_leaderelection value: &quot;true&quot; - name: vip_leaseduration value: &quot;5&quot; - name: vip_renewdeadline value: &quot;3&quot; - name: vip_retryperiod value: &quot;1&quot; - name: address value: ${VIP} - name: prometheus_server value: :2112 - name: lb_enable value: &quot;true&quot; image: ghcr.io/kube-vip/kube-vip:v0.5.12 imagePullPolicy: Always name: kube-vip securityContext: capabilities: add: - NET_ADMIN - NET_RAW hostNetwork: true serviceAccountName: kube-vip tolerations: - effect: NoSchedule operator: Exists - effect: NoExecute operator: Exists EOF  "},{"title":"K3s installation - Control-plane nodes​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s-installation---control-plane-nodes","content":"Once kube-vip is in place, the rest of the control-plane nodes can be added to the cluster by pointing them to the VIP as: export VIP=192.168.205.10 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;server --server https://${VIP}:6443 --write-kubeconfig-mode=644&quot; K3S_TOKEN=foobar sh -  NOTE: For a real HA scenario, it is required for etcd to have an odd number of nodes, so it would be required to add two more control plae nodes. After a while, the nodes will join the cluster successfully and an HA cluster will be ready. "},{"title":"Kubeconfig tweaks​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#kubeconfig-tweaks","content":"The kubeconfig file that is generated as part of the installation has localhost as the Kubernetes API endpoint, so in order to use it from outside, it needs to be changed to the VIP as: scp 192.168.205.10:/etc/rancher/k3s/k3s.yaml ~/.kube/config &amp;&amp; sed -i '' 's/127.0.0.1/192.168.205.10/g' ~/.kube/config &amp;&amp; chmod 600 ~/.kube/config  "},{"title":"K3s installation - adding agents​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#k3s-installation---adding-agents","content":"Agents can be added as usual, pointing to the VIP address as: export VIP=192.168.205.10 curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC=&quot;agent --server https://${VIP}:6443&quot; K3S_TOKEN=foobar sh -  "},{"title":"Final picture​","type":1,"pageTitle":"Intro","url":"/quickstart/k3s-on-slemicro#final-picture","content":"kubectl get nodes -o jsonpath=&quot;{.items[*].status.addresses[?(@.type=='InternalIP')].address}&quot; 192.168.205.69 192.168.205.70 192.168.205.71 192.168.205.72 192.168.205.73% kubectl cluster-info Kubernetes control plane is running at https://192.168.205.10:6443  As you can see, the control plane IP is the VIP and the nodes have their own IP. Sweet! "},{"title":"Intro","type":0,"sectionRef":"#","url":"/quickstart/elemental-utm-aarch64","content":"","keywords":""},{"title":"Elemental​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#elemental","content":"Via the official docs: Elemental is a software stack enabling a centralized, full cloud-native OS management with Kubernetes. The Elemental Stack consists of some packages on top of SLE Micro for Rancher: elemental-toolkit - includes a set of OS utilities to enable OS management via containers. Includes dracut modules, bootloader configuration, cloud-init style &gt; configuration services, etc.elemental-operator - this connects to Rancher Manager and handles MachineRegistration and MachineInventory CRDselemental-register - this registers machines via machineRegistrations and installs them via elemental-clielemental-cli - this installs any elemental-toolkit based derivative. Basically an installer based on our A/B install and upgrade systemrancher-system-agent - runs on the installed system and gets instructions (&quot;Plans&quot;) from Rancher Manager what to install and run on the system Cluster Node OSes are built and maintained via container images through the Elemental CLI and they can be installed on new hosts using the Elemental UI plugin for &gt; Rancher Manager or the Elemental CLI. The Elemental Operator and the Rancher System Agent enable Rancher Manager to fully control Elemental clusters, from the installation and management of the OS on the &gt; Nodes to the provisioning of new K3s or RKE2 clusters in a centralized way. What is Elemental Teal ?​ Elemental Teal is the combination of &quot;SLE Micro for Rancher&quot; with the Rancher Elemental stack. SLE Micro for Rancher is a containerized and &quot;stripped to the bones&quot; operating system layer. At its core, it only requires grub2, dracut, a kernel, and systemd. Its sole purpose is to run Kubernetes (k3s or RKE2), with everything controlled through Rancher Manager. Elemental Teal is built in the openSUSE Build Service and available through the openSUSE Registry. "},{"title":"Elemental on OSX on Apple Silicon​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#elemental-on-osx-on-apple-silicon","content":"Elemental is a Kubernetes thing, so it only requires a proper cluster up &amp; running. However, in order to provision real clusters and hosts, it requires to perform some steps such as downloading and customizing an ISO (or an image file) and boot the ISO. This quickstart uses UTM to create a VM and a few steps to create a proper image to boot from. The trick here is there is no ARM64 image yet, but just a Raspberry Pi one... so that's the one we will use. It is not generic, but it works. "},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#prerequisites","content":"A Kubernetes cluster where Elemental is deployed. Hint, you can use the K3s on SLE Micro guide.Rancher server configured (server-url set). Hint: you can use the official Rancher docs or the create_vm.sh script for inspiration.Helmjq "},{"title":"Elemental UI Rancher extension​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#elemental-ui-rancher-extension","content":"This is an optional step to enable the Elemental UI extension in Rancher (see more about Rancher extensions): helm repo add rancher-charts https://charts.rancher.io/ helm upgrade --create-namespace -n cattle-ui-plugin-system --install ui-plugin-operator rancher-charts/ui-plugin-operator helm upgrade --create-namespace -n cattle-ui-plugin-system --install ui-plugin-operator-crd rancher-charts/ui-plugin-operator-crd # Wait for the operator to be up while ! kubectl wait --for condition=ready -n cattle-ui-plugin-system $(kubectl get pods -n cattle-ui-plugin-system -l app.kubernetes.io/instance=ui-plugin-operator -o name) --timeout=10s; do sleep 2 ; done # Deploy the elemental UI plugin # NOTE: TABs and then spaces... cat &lt;&lt;- FOO | kubectl apply -f - apiVersion: catalog.cattle.io/v1 kind: UIPlugin metadata: name: elemental namespace: cattle-ui-plugin-system spec: plugin: endpoint: https://raw.githubusercontent.com/rancher/ui-plugin-charts/main/extensions/elemental/1.1.0 name: elemental noCache: false version: 1.1.0 FOO # Or # helm repo add rancher-ui-plugins https://raw.githubusercontent.com/rancher/ui-plugin-charts/main # helm upgrade --install elemental rancher-ui-plugins/elemental --namespace cattle-ui-plugin-system --create-namespace  After a while, the plugin will be shown in the UI as:  "},{"title":"Elemental Operator​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#elemental-operator","content":"Elemental is managed by an operator deployed via Helm as: helm upgrade --create-namespace -n cattle-elemental-system --install --set image.imagePullPolicy=Always elemental-operator oci://registry.opensuse.org/isv/rancher/elemental/dev/charts/rancher/elemental-operator-chart  The values.yaml file have some variables interesting to see After a few seconds you should see the operator pod appear on the cattle-elemental-system namespace: kubectl get pods -n cattle-elemental-system NAME READY STATUS RESTARTS AGE elemental-operator-64f88fc695-b8qhn 1/1 Running 0 16s  "},{"title":"Kubernetes resources​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#kubernetes-resources","content":"Based on the Elemental quickstart guide, a few Kubernetes resources need to be created. NOTE: It is out of the scope of this document to provide an explanation about the resources managed by Elemental, however the official documentation explains all those in good detail. cat &lt;&lt;- EOF | kubectl apply -f - apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate metadata: name: my-machine-selector namespace: fleet-default spec: template: spec: selector: matchExpressions: - key: location operator: In values: [ 'europe' ] EOF cat &lt;&lt;- EOF | kubectl apply -f - kind: Cluster apiVersion: provisioning.cattle.io/v1 metadata: name: my-cluster namespace: fleet-default spec: rkeConfig: machineGlobalConfig: etcd-expose-metrics: false profile: null machinePools: - controlPlaneRole: true etcdRole: true machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: my-machine-selector name: pool1 quantity: 1 unhealthyNodeTimeout: 0s workerRole: true machineSelectorConfig: - config: protect-kernel-defaults: false registries: {} kubernetesVersion: v1.24.8+k3s1 EOF cat &lt;&lt;- 'EOF' | kubectl apply -f - apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: root passwd: root elemental: install: reboot: true device: /dev/vda debug: true disable-boot-entry: true registration: emulate-tpm: true machineInventoryLabels: manufacturer: &quot;${System Information/Manufacturer}&quot; productName: &quot;${System Information/Product Name}&quot; serialNumber: &quot;${System Information/Serial Number}&quot; machineUUID: &quot;${System Information/UUID}&quot; EOF  This creates a MachineRegistration object which will provide a unique URL which will be used with elemental-register to register the node during installation, so the operator can create a MachineInventory which will be using to bootstrap the node. See that the label has been see to match the selector here already, although it can always be added later to the MachineInventory.  NOTE: At this point the x86_64 and ARM64 quickstart differs because for x86_64 there is a SeedImage object that needs to be created and that doesn't exist for ARM64 (yet). "},{"title":"Preparing the installation image​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#preparing-the-installation-image","content":"Elemental's support for Raspberry Pi is primarily for demonstration purposes at this point. Therefore the installation process is modelled similar to x86-64. You boot from a seed image (USB stick in this case) and install to a storage medium (SD-card for Raspberry Pi). First step is to download the machineregistration object that will instruct where to get the config for the node to be installed: curl -k $(kubectl get machineregistration -n fleet-default my-nodes -o jsonpath=&quot;{.status.registrationURL}&quot;) -o livecd-cloud-config.yaml   Then, the rpi.raw image is downloaded and checked the integrity just to be safe: curl -Lk https://download.opensuse.org/repositories/isv:/Rancher:/Elemental:/Stable:/Teal53/images/rpi.raw -o rpi.raw curl -Lk https://download.opensuse.org/repositories/isv:/Rancher:/Elemental:/Stable:/Teal53/images/rpi.raw.sha256 -o rpi.raw.sha256 sha256sum -c rpi.raw.sha256  Finally, the livecd-cloud-config.yaml file is injected in the vanilla rpi.raw image: IMAGE=rpi.raw DEST=$(mktemp -d) SECTORSIZE=$(sfdisk -J ${IMAGE} | jq '.partitiontable.sectorsize') DATAPARTITIONSTART=$(sfdisk -J ${IMAGE} | jq '.partitiontable.partitions[1].start') mount -o rw,loop,offset=$((${SECTORSIZE}*${DATAPARTITIONSTART})) ${IMAGE} ${DEST} mv livecd-cloud-config.yaml ${DEST}/livecd-cloud-config.yaml umount ${DEST}  NOTE: The rpi.raw image has two partitions. RPI_BOOT contains the boot loader files and COS_LIVE the Elemental files, where the livecd-cloud-config.yaml file needs to be copied. The easiest way to run a K3s HA cluster is by installing a first node using the --cluster-init flag and then, start adding nodes. "},{"title":"UTM VM​","type":1,"pageTitle":"Intro","url":"/quickstart/elemental-utm-aarch64#utm-vm","content":"Then, a new UTM VM needs to be created and the rpi.raw file configured as USB.    Map the raw file as an ISO and configure the hardware as you please:  Set a proper name:  Finally, it is needed to configure the raw disk as USB:   NOTE: The operating system disk device should be the first one, then the USB, so the USB will boot just once as a fallback. After a while, a new machineinventory host will be present: kubectl get machineinventory -n fleet-default m-ed0a3f46-d6f8-4737-9884-e3a898094994 -o yaml apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventory metadata: annotations: elemental.cattle.io/registration-ip: 192.168.205.106 creationTimestamp: &quot;2023-05-03T14:04:56Z&quot; generation: 1 labels: machineUUID: ec49ff2a-e14f-42bf-8098-4162f14ee1f9 manufacturer: QEMU productName: QEMU-Virtual-Machine serialNumber: Not-Specified name: m-ed0a3f46-d6f8-4737-9884-e3a898094994 namespace: fleet-default resourceVersion: &quot;15848&quot; uid: 79608121-034d-4d64-8b48-6624607bbadd spec: tpmHash: a2e5b231dac4e90151454e2ebc76a6b118f7d1b826b810d22868b2d09b38b7f7 status: conditions: - lastTransitionTime: &quot;2023-05-03T14:07:45Z&quot; message: plan successfully applied reason: PlanSuccessfullyApplied status: &quot;True&quot; type: Ready - lastTransitionTime: &quot;2023-05-03T14:04:56Z&quot; message: Waiting to be adopted reason: WaitingToBeAdopted status: &quot;False&quot; type: AdoptionReady plan: checksum: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a secretRef: name: m-ed0a3f46-d6f8-4737-9884-e3a898094994 namespace: fleet-default state: Applied  Finally, labeling the machineinventory of the discovered new host will trigger the installation: kubectl -n fleet-default label machineinventory $(kubectl get machineinventory -n fleet-default --no-headers -o custom-columns=&quot;:metadata.name&quot;) location=europe   kubectl get cluster -n fleet-default NAME READY KUBECONFIG my-cluster true my-cluster-kubeconfig  Profit! kubectl get secret -n fleet-default my-cluster-kubeconfig -o jsonpath='{.data.value}' | base64 -d &gt;&gt; ~/my-cluster-kubeconfig KUBECONFIG=~/my-cluster-kubeconfig kubectl get nodes NAME STATUS ROLES AGE VERSION m-ed0a3f46-d6f8-4737-9884-e3a898094994 Ready control-plane,etcd,master,worker 6m25s v1.24.8+k3s1 KUBECONFIG=~/my-cluster-kubeconfig kubectl get pods -A NAMESPACE NAME READY STATUS RESTARTS AGE cattle-fleet-system fleet-agent-7ffcdff7c5-2rvvl 1/1 Running 0 2m47s cattle-system apply-system-agent-upgrader-on-m-ed0a3f46-d6f8-4737-9884-1jhpkx 0/1 Completed 0 2m1s cattle-system cattle-cluster-agent-684c4687c8-scgvb 1/1 Running 0 61s cattle-system helm-operation-hjkcr 0/2 Completed 0 5m35s cattle-system rancher-webhook-85bb446df8-r8g6r 1/1 Running 0 5m22s cattle-system system-upgrade-controller-65bcf49944-rp2gr 1/1 Running 0 2m47s kube-system coredns-7b5bbc6644-2zdlk 1/1 Running 0 6m20s kube-system helm-install-traefik-crd-ksm4q 0/1 Completed 0 61s kube-system helm-install-traefik-kg4qv 0/1 Completed 0 61s kube-system local-path-provisioner-687d6d7765-j54vp 1/1 Running 0 6m20s kube-system metrics-server-84f8d4c4fc-6t6kc 1/1 Running 0 6m20s kube-system svclb-traefik-7ca8393f-gvdcc 2/2 Running 0 5m58s kube-system traefik-6b8f69d897-bwtgq 1/1 Running 0 5m58s   "},{"title":"Intro","type":0,"sectionRef":"#","url":"/quickstart/slemicro-utm-aarch64","content":"","keywords":""},{"title":"OSX Virtualization​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#osx-virtualization","content":"Virtualization of Linux hosts on OSX can be achieved with various tools. There are commercial products such as VMWare Fusion or Parallels Desktop as well as open-source projects such as VirtualBox, UTM or Lima. UTM is an OSX application that uses QEMU under the hood and offers a GUI to manage the VM lifecycle. It supports Apple silicon CPUs, and it can use native OSX virtualization (Virtualization.framework) as well. It also has a scripting interface via Apple Script to automate some processes and a proper CLI (utmctl) is on the works. Lima is based on QEMU (experimental support for Virtualization.framework) as well and it launches Linux virtual machines with automatic file sharing and port forwarding (like WSL2), and containerd. Lima is expected to be used on macOS hosts, but can be used on Linux hosts as well. Lima has a proper CLI tool (limactl) and the best part is VMs can be defined in yaml files, so you can even deploy K8s clusters with just a single command (see https://github.com/lima-vm/lima/blob/master/examples/k8s.yaml) NOTE: Rancher desktop is based on Lima However, Lima doesn't support SLE Micro (yet) as Lima customizes the VM at boot to install some packages and services and SLE Micro uses a different approach to those things (for example as it is immutable, it requires installing packages using ignition/combustion) "},{"title":"SLE Micro installation automation: ISO vs Image​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#sle-micro-installation-automation-iso-vs-image","content":"SLE Micro can be installed traditionally using an ISO file that boots once and using click-ops you can customize it as you wish (see https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#cha-install) but that won't be useful. ISO installation can be customized using boot parameters (see https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#sec-boot-parameters-list) but those don't cover all the options. However, ISO based installation supports using AutoYaST (see https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-autoyast/) to automate the installation process. SLE Micro can be also deployed using pre-built images. Currently, there are two types of images available: raw disk images and selfinstall ISOs. SLE Micro raw images are delivered for the AMD64/Intel 64 architecture, IBM Z ZSeries and also AArch64, however the selfinstall images are currently delivered only for the AMD64/Intel 64 architecture. The pre-built images (both selfinstall ISOs and raw disk images) are intended to be configured on the first boot by using either Ignition or Combustion. To summarize, the two ways as of today to deploy SLE Micro on Aarch64 on an automated fashion would be using the ISO + AutoYaST or raw images + Ignition/Combustion. "},{"title":"Ignition vs Butane vs Combustion​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#ignition-vs-butane-vs-combustion","content":"Ignition is a provisioning tool that enables you to configure a system according to your specification on the first boot. When the system is booted for the first time, Ignition is loaded as part of an initramfs and searches for a configuration file within a specific directory (on a USB flash disk, or you can provide a URL). All changes are performed before the kernel switches from the temporal file system to the real root file system (before the switch_root command is issued). Ignition uses a configuration file in the JSON format. The file is called config.ign. SLE Micro supportsIgnition config spec 3.3.0(seehttps://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#sec-ignition-configurationfor more information). Ignition files can be complex to generate manually (specially for the file permissions syntax in hex or multiline things) so you can useopensuse.github.io/fuel-ignitionto help you generate a basic one. Butane is a more human readable (and writable) configuration syntax based on yaml that can be translated to Ignition easily with the butane CLI as Butane is not consumable by Ignition. Combustion is a dracut module that enables you to configure your system on its first boot. Combustion reads a provided file called script and executes commands in it and thus performs changes to the file system. You can use Combustion to change the default partitions, set users\\' passwords, create files, install packages, etc. The Combustion dracut module is invoked after the ignition.firstboot argument is passed to the kernel command line. Combustion then reads the configuration from script. Combustion tries to configure the network, if the network flag has been found in script. After /sysroot is mounted, Combustion tries to activate all mount points in /etc/fstab and then call transactional-update to apply other changes (like setting root password or installing packages). See https://documentation.suse.com/sle-micro/5.3/single-html/SLE-Micro-deployment/#sec-combustion-scriptfor more information. "},{"title":"Ignition/Combustion and UTM​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#ignitioncombustion-and-utm","content":"Ignition and Combustion are intended to automate the deployment of SLE Micro systems. To use them with UTM there are a couple of alternatives: Use the QEMU fw_cfg flag as -fw_cfg name=opt/org.opensuse.combustion/script,file=/var/combustion-scriptfor combustion or -fw_cfg name=opt/com.coreos/config,file=PATH_TO_config.ign for ignition Create a raw disk or ISO file to host the Ignition or Combustion (or both) files. For Ignition, the configuration file config.ign must reside in theignition subdirectory on the configuration media labeled ignition. The directory structure must look as follows: &lt;root directory&gt; └── ignition └── config.ign  For Combustion, the configuration device needs to be named combustion, a specific directory structure in that configuration medium needs to be created and include a configuration file named script. In the root directory of the configuration medium, create a directory calledcombustion and place the script into this directory along with other files---SSH key, configuration files, etc. The directory structure then should look as follows: &lt;root directory&gt; └── combustion └── script └── other files  Combustion can be used along with Ignition. If you intend to do so, label your configuration medium ignition and include the ignitiondirectory with the config.ign to your directory structure as shown below: &lt;root directory&gt; └── combustion └── script └── other files └── ignition └── config.ign  In this scenario, Ignition runs before Combustion. Image-based process step by step NOTE: There is a helper script that automates all the steps included herehttps://github.com/suse-edge/misc/blob/main/slemicro/create_vm.sh "},{"title":"Prerequisites​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#prerequisites","content":"SLE Micro raw image Download the raw image file from the SUSE website at https://www.suse.com/download/sle-micro/ Select ARM architectureLook for the raw file (I.e.- SLE-Micro.aarch64-5.3.0-Default-GM.raw.xz) NOTE: You need to have a valid user on the SUSE site to be able to download the file. Access to https://scc.suse.com/ to generate a registration code Butane, qemu-img and cdrtools installed (using brew for example) brew install butane cdrtools qemu-img UTM installed (using brew for example) brew install --cask utm  Note: If using the previous script, it is required to install UTM 4.2.2 at least as it includes the proper support for the automation. "},{"title":"Image preparation​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#image-preparation","content":"Uncompress the SLE Micro image xz -d ~/Downloads/SLE-Micro.aarch64-5.3.0-Default-GM.raw.xz Move the file to a proper location and rename it to fit the VM hostname cp ~/Downloads/SLE-Micro.aarch64-5.3.0-Default-GM.raw ~/VMs/slemicro.raw Resize the image file. In this example, to 30G qemu-img resize -f raw ~/VMs/slemicro.raw 30G &gt; /dev/null  "},{"title":"Ignition & Combustion files​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#ignition--combustion-files","content":"To automate the installation, we will leverage Butane, Ignition and Combustion as explained before: Create a temporary folder to store the assets TMPDIR=$(mktemp -d) Create the required folders for ignition and combustion mkdir -p ${TMPDIR}/{combustion,ignition} Create a config.fcc butane config file as required. See the following example to set a root password for the root user, and to configure the hostname to be &quot;slemicro&quot;' cat &lt;&lt; EOF &gt; ${TMPDIR}/config.fcc variant: fcos version: 1.4.0 storage: files: - path: /etc/hostname mode: 0644 overwrite: true contents: inline: &quot;slemicro&quot; passwd: users: - name: root password_hash: &quot;$y$j9T$/t4THH10B7esLiIVBROsE.$G1lyxfy/MoFVOrfXSnWAUq70Tf3mjfZBIe18koGOuXB&quot; EOF Create a script combustion file as required. See the following example to register the SLE Micro instance to SUSE's SCC (use your own email/regcode) and to create a /etc/issue.d/combustion file cat &lt;&lt; EOF &gt; ${TMPDIR}/combustion/script #!/bin/bash # combustion: network # Redirect output to the console exec &gt; &gt;(exec tee -a /dev/tty0) 2&gt;&amp;1 # Set hostname at combustion phase for SUSEConnect hostname slemicro # Registration if ! which SUSEConnect &gt; /dev/null 2&gt;&amp;1; then zypper --non-interactive install suseconnect-ng fi SUSEConnect --email foobar@suse.com --url https://scc.suse.com --regcode YOURCODE # Leave a marker echo &quot;Configured with combustion&quot; &gt; /etc/issue.d/combustion EOF Convert the butane config to ignition butane -p -o ${TMPDIR}/ignition/config.ign ${TMPDIR}/config.fcc Create an ISO file. It is requried for both ignition and combustion to work that the ISO is labeled as ignition (hence the -V parameter) mkisofs -full-iso9660-filenames -o ignition-and-combustion.iso -V ignition ${TMPDIR} Optional: Remove the temporary folder rm -Rf ${TMPDIR}  "},{"title":"VM Creation​","type":1,"pageTitle":"Intro","url":"/quickstart/slemicro-utm-aarch64#vm-creation","content":"Now it is time to finally use UTM to boot the VM  Create a New Virtual Machine using Virtualization  Select &quot;Other&quot;  Enable the &quot;Skip ISO boot&quot; option as we will use the raw disk directly  Select the required CPU/RAM:  Accept the storage size as it is, it will be deleted before booting it  Skip the Shared Directory  Edit the VM name and enable the &quot;Open VM Settings&quot; toggle to customize it further:  Delete the VirtIO Drive  Add a new Device and select &quot;Import&quot;  Select the raw image file (~/VMs/slemicro.raw in this case)  Repeat the last two steps to add the ignition-and-combustion.iso file  Configure the ISO as Read Only and &quot;CD/DVD (ISO) Image&quot;  Finally, boot the VM. After a couple of seconds, the VM will boot up and will configure itself using the ignition and combustion scripts, including registering itself to SCC   NOTE: In case the VM doesn't get network connectivity, tryhttps://github.com/utmapp/UTM/discussions/3530#discussioncomment-5072113 ISO Process (TBD) Download the ISO fileCreate a new VM on UTM using the ISO fileCreate the autoyast answer fileUse the AutoYaST boot parameter to map to the answer fileBoot the VMProfit! "}]